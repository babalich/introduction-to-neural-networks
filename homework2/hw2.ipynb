{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в искусственные нейронные сети\n",
    "# Урок 3. Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Содержание методического пособия:\n",
    "\n",
    "\n",
    "<ol>\n",
    "<li>Способы создания нейросетей</li>\n",
    "<li>Что такое Keras</li>\n",
    "<li>Основы синтаксиса</li>\n",
    "<li>Простая нейросеть на Keras</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Способы создания нейросетей\n",
    "\n",
    "Нейросети это математические модели. Программирую на любом языке можно решать задачи связанные с математикой. Однако встает вопрос какой язык подойдет для этого больше? Не считая учебных нейросетей, нейросети как правило работают с большим количеством данных. Поэтому, чтобы обучение нейросетей происходило с приемлимой скоростью нужно использовать быстрый язык. Например Си. Но так как язык Си это язык с низким уровнем абстракции то программировать и модифицировать на нем нейросети было бы крайне затруднительно. \n",
    "\n",
    "Хорошо может подойти для этих целей язык Python. Так как он с одной стороны имеет высокий уровень абстракции с другой стороны операции с массивами данных могут сделать его библиотеки написанные на Си. Таким способом мы пользовались на первых 2 уроках. Однако если писать нейросети таким образом то будет много повторяющегося кода поскольку архитектуры нейросетей остаются одинаковыми и зачастую у них только меняются параметры. Кроме этого нам может понадобиться хорошо знать архитектуры самых разных нейронных сетей чтобы реализовать их вручную. Работа таким образом затруднительна для людей не имеющих достаточной подготовки, а для имеющих может быть нааборот рутиной.\n",
    "\n",
    "Существуют фреймворки для созданий нейронных сетей. Они являются, пожалуй основным рабочим способом создания нейронных сетей. Вот их неполный перечень:\n",
    "\n",
    "1. TensorFlow\n",
    "2. PyTorch\n",
    "3. Keras\n",
    "4. Microsoft Cognitive Toolkit (CNTK)\n",
    "5. Caffe\n",
    "6. Apache MXNet\n",
    "\n",
    "Упрощение создания нейронных сетей не заканчивается на этих фрейворках. Существуют инструменты которые позволяют создавать нейронные сети без навыков программирования, строя нейросети графически. Примеры: Neural Designer, Deep Learning Studio.\n",
    "\n",
    "Но и на этом не заканчиваются способы создания нейросетей. Существуют инструменты самостоятельно создающие нейронные сети. Это так называемые AutoML инструменты. Вот примеры популярных из них:\n",
    "1. MLBox\n",
    "2. TPOT\n",
    "3. Autokeras\n",
    "\n",
    "Как вы возможно заметили что все эти инструменты отранжированы походы изложения в порядке возрастания уровня абстракции. Соответсвенно говоря о плюсах минусах того или иного инструмента мы должны понимать в принципе плюсы минусы повышения уровня абстракции. Чем он выше тем меньше производительность и тем меньше его гибкость и набоорот.\n",
    "\n",
    "Как уже было сказано наиболее востребованных в рабочих целях является тот уровень абстракции, который дают фреймворки. Будем изучать дальше и пользовать ими. Остается сделать выбор среди них. Самый популярный фреймворк для создания нейросетей TensorFlow. Самый популярный для обучения - Keras. На этом уроке мы изучим с вами Keras, а на следующим TensorFlow. Также стоит отметить, что эти фреймворки взаимосвязаны - Keras как правило работает поверх TensorFlow, а сам TensorFlow позволяет пользовать средствами Keras при необходимости.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что такое Keras\n",
    "\n",
    "Keras появился относительно недавно - в 2015 г. Но за это время стал одним из самых популярных фреймоворков для создания нейросетей и фактически стандартом для использования его начинающими.\n",
    "\n",
    "В чем причина его популярности? Keras позволяет создовать на высоком уровне абстракции. Т.е. на не нужно вручную реализовать с помощью математикаподобного кода те или иные элементы нейронной сети. Мы можем оперировать слоями, количеством нейронов в них, выбором функции активации и т.д. В тоже время keras содержит инструментарий для всего того, что может понадобиться для работы - например ряд встроенных датасетов, возможность обрабатывать изображения.\n",
    "\n",
    "В техническом плане Keras это оболочка над инструментами меньшей степени абстракции. На выбор он может работать поверх TensorFlow, Microsoft Cognitive Toolkit, R, Theano, PlaidML.\n",
    "\n",
    "Keras пользуется также на соревнованиях Kaggle.\n",
    "\n",
    "Однако стоит отметить, что в реальных проектах чаще используется TensorFlow, который мы будем изучать в след. уроках.\n",
    "\n",
    "Keras как и любой высокобастрактный инструмент имеет изъяны в качестве меньшей гибкостью и производительснотью чем тот же tensorflow.\n",
    "\n",
    "Стоит также отметить, что Google официально поддерживает Keras, его автор François Chollet, является сотрудником Google. TensorFlow сам в свою очередь позволяет использовать возможности Keras, т.е. в нем заложена возможность переходить на более высокой уровень абстракции.\n",
    "\n",
    "В данном уроке мы с вами рассмотрим пример обучения нейронной сети с помощью Keras. Но прежде давайте посмотрим на основы синтаксиса Keras и стандартные задачи, которые нужно выполнить при обучении нейронной сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основы синтаксиса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Установка и работа с данными**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала необходимо установить keras. Надо полагать вы хорошо знакомы с командой pip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sudo python3 pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попробуем получить датасет mnist и проанализировать его содержимое.\n",
    "Это еще не будет синтаксис Keras, но это часто встречающаяся задача. Не обращайте внимание на предупреждения от TensorFlow. Их часто бывает много и их можно подавить при необходимости."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    import numpy as np\n",
    "    import mnist\n",
    "    import keras\n",
    "\n",
    "    # The first time you run this might be a bit slow, since the\n",
    "    # mnist package has to download and cache the data.\n",
    "    train_images = mnist.train_images()\n",
    "    train_labels = mnist.train_labels()\n",
    "\n",
    "    print(train_images.shape) # (60000, 28, 28)\n",
    "    print(train_labels.shape) # (60000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что в данном случае мы смогли с вами узнать? Что тренировочный датасет mnist состоит из 60000 изображений 28 на 28 пикселей. Такие небольшие датасеты с маленькими изображениями встретятся вам и в других учебных датасетах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что нам нужно делать теперь? Если скаченный нами датасет не имеет разделения на тренировочный и тестовый то поделить их. В нашем случае наш тренировочный датасет состоит из 60 000 изображений и тестовый из 10 000 и они поделены по умолчанию.\n",
    "\n",
    "Нам теперь нужно конверитировать значения пикселей из вида от 1 до 255 в набор значений от -0.5 до 0.5.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mnist\n",
    "\n",
    "train_images = mnist.train_images()\n",
    "train_labels = mnist.train_labels()\n",
    "test_images = mnist.test_images()\n",
    "test_labels = mnist.test_labels()\n",
    "\n",
    "# Normalize the images.\n",
    "train_images = (train_images / 255) - 0.5\n",
    "test_images = (test_images / 255) - 0.5\n",
    "\n",
    "# Flatten the images.\n",
    "train_images = train_images.reshape((-1, 784))\n",
    "test_images = test_images.reshape((-1, 784))\n",
    "\n",
    "print(train_images.shape) # (60000, 784)\n",
    "print(test_images.shape)  # (10000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Создание модели**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После первичной подготовки данных дальше как правило следует создание модели нейронной сети, которая будет учиться на этих данных.\n",
    "\n",
    "Ниже типичный код учебной нейросети - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # define the keras model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте разберемся с теми командами, которые нам встетились в этом коде.\n",
    "\n",
    "Sequential - позволяет создать нейросети где слои имеют форму стека. Сигнал в них передается от одного слоя к другому. В противовес этой разновидности есть нейросети где сигнал может не сразу передаваться в следующий слой а попадать в цикл. Такие нейросети мы разберем в следующих уроках.\n",
    "\n",
    "Dense - позволяет каждому нейронну быть связанному с другим нейронном. В противовес этом может быть необходимость не делать так много связей. Неполносвязнные архитектуры мы также разберем на этом курсе, они основа компьютерного зрения.\n",
    "\n",
    "Цифры 12, 8, 1 обозначают количество нейронов в каждом конкретном слое\n",
    "\n",
    "Activation - позволяет определить формулу по которой будет активироваться нейрон."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Компиляция модели**\n",
    "\n",
    "На этапе компиляции модель с заданными параметрами ранее создается. Вот типичный учебный пример:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "    # создание keras модели\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако на этой стадии мы должны сделать еще некоторые настройки нейронной сети. Разберем команды из кода выше.\n",
    "\n",
    "loss - позволяет задать формулы по которой будет определяться степень ошибки нейронной сети.\n",
    "\n",
    "optimizer - позволяет задать алгоритм, который будет осуществлять изменения весов по всей нейронной сети (backpropagation)\n",
    "\n",
    "metrics - позволяет опредилить кретирии по которым будет оцениваться степень обученности нейросети.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Передача данных для обучения нейросети**\n",
    "\n",
    "После того как нейросеть создана можно передавать ей данные для обучения. Ниже типичный пример кода для этого.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # передача обучающего датасета keras модели\n",
    "    model.fit(X, y, epochs=150, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберем команды из этого примера.\n",
    "X, y - содержат все обучающие данные\n",
    "epochs - определяет сколько раз через нейросеть должен пройти весь набор данных\n",
    "bath_size - определяет количество обучающих примеров передающихся нейросети на каждой итерации обучения.\n",
    "verbose - позволяет определять информацию, котору вы видете во время обучения нейронной сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оценка обученности нейронной сети**\n",
    "\n",
    "Следующей стадией может быть проверка обученности нейронной сети. Команда Keras для этих целей - \n",
    "\n",
    "    results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "    \n",
    "В данном случае мы просто указываем какую модель на каких данных мы хотим проверить"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Запуск нейронной сети для выполнения работы**\n",
    "\n",
    "На этой стадии мы можем попробовать запустить нейронную сеть на данных которые мы хотели бы чтобы она оценила. Осуществить распознования объекта на фотографии например.\n",
    "Вот код для этих целей - \n",
    "\n",
    "    predictions = model.predict(x_test[:3])\n",
    "    \n",
    "В качестве аргумента здесь указывается массив даныхх содержащих, например фотографию в виде массива чисел.    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы с вами рассмотрели основные стадии процесса обучения нейросети и команды Keras, для этого. Безусловно здесь приведен далеко неполный перечень возможностей Keras. У Keras есть также возможность сохранять созданную нейросеть, запускать уже имеющиюся, различные средства для создания нейросетей разных архитектур и другое. С чем то из арсенала Keras мы с вами познакомимся по ходу курса, а с остальным вы можете познакомиться на сайте Keras в разделе документация."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Простая нейросеть на Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попрубуем сделать нейросеть на Keras использую полученные выше знания. Попробуем обучить нейросеть различать рукописные цифры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# The full neural network code!\n",
    "###############################\n",
    "import numpy as np\n",
    "import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "train_images = mnist.train_images()\n",
    "train_labels = mnist.train_labels()\n",
    "test_images = mnist.test_images()\n",
    "test_labels = mnist.test_labels()\n",
    "\n",
    "# Normalize the images.\n",
    "train_images = (train_images / 255) - 0.5\n",
    "test_images = (test_images / 255) - 0.5\n",
    "\n",
    "# Flatten the images.\n",
    "train_images = train_images.reshape((-1, 784))\n",
    "test_images = test_images.reshape((-1, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Pavel\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Pavel\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.4336 - accuracy: 0.8725\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.2100 - accuracy: 0.9374\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1573 - accuracy: 0.9527\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1238 - accuracy: 0.9624\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1069 - accuracy: 0.9667\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.0928 - accuracy: 0.9717\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0810 - accuracy: 0.9745\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0747 - accuracy: 0.9766\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.0671 - accuracy: 0.9780\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0607 - accuracy: 0.9807\n",
      "10000/10000 [==============================] - 0s 37us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09448125197566114, 0.9733999967575073]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Dense(64, activation='elu', input_shape=(784,)),\n",
    "  Dense(64, activation='elu'),\n",
    "  Dense(64, activation='elu'),\n",
    "  Dense(10, activation='softplus'),\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=10,\n",
    "  batch_size=32*4,\n",
    ")\n",
    "\n",
    "# Evaluate the model.\n",
    "model.evaluate(\n",
    "  test_images,\n",
    "  to_categorical(test_labels)\n",
    ")\n",
    "\n",
    "# Save the model to disk.\n",
    "#model.save_weights('model.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('model.h5')\n",
    "\n",
    "# # Predict on the first 5 test images.\n",
    "# predictions = model.predict(test_images[:5])\n",
    "\n",
    "# # Print our model's predictions.\n",
    "# print(np.argmax(predictions, axis=1)) # [7, 2, 1, 0, 4]\n",
    "\n",
    "# # Check our predictions against the ground truths.\n",
    "# print(test_labels[:5]) # [7, 2, 1, 0, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практическое задание\n",
    "\n",
    "<ol>\n",
    "    <li>Попробуйте обучить, нейронную сеть на Keras(рассмотренную на уроке) на датасете MNIST с другими параметрами. \n",
    "        Опишите в комментарии к уроку - какой результата вы добились от нейросети? Что помогло вам улучшить ее точность?</li>\n",
    "    <li>Поработайте с документацией Keras. Попробуйте найти полезные команды Keras неразобранные на уроке.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9734"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = np.argmax(model.predict(test_images), axis=1)\n",
    "accuracy_score(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наш исходный результат: 0.9677"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Увеличили число эпох с 5 до 10: 0.9694 <br>\n",
    "2. Добавили слой: 0.9706 <br>\n",
    "3. Увеличили кол-во нейронов до 76: 0.9682 - снижение результата, вернем как было <br>\n",
    "4. Сменили функцию активации: elu - 0.9708, tanh: 0.966, LeakyReLU(0.1): 0.9729 с увеличение времени работы.\n",
    "5. batch = 32*4: 0.9762 <br>\n",
    "6. Замена softplus на softmax: 0.9714, оставим softplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 59,210\n",
      "Trainable params: 59,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Поработайте с документацией Keras. Попробуйте найти полезные команды Keras неразобранные на уроке. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следуя своей логике из предыдущего задания, хотелось попробовать какой-нибудь другой слой вместо Dense. В итоге случайно в справочнике по функциям активации (https://keras.io/api/layers/activations/#elu-function) увидел Conv2D и помощью статьи https://habr.com/ru/post/454986/ понял, что оказывается это сверточная нейросеть. <br>\n",
    "Основная сложность - это понять, какие подходы эффективны для разных типов данных, а так же размерность данных, которые мы передаем в модель на разных этапах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = mnist.train_images()\n",
    "train_labels = mnist.train_labels()\n",
    "test_images = mnist.test_images()\n",
    "test_labels = mnist.test_labels()\n",
    "\n",
    "# Normalize the images.\n",
    "train_images = (train_images / 255) - 0.5\n",
    "test_images = (test_images / 255) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_images.shape[1], train_images.shape[2] \n",
    "n_classes = np.unique(train_labels).size\n",
    "train_size = train_images.shape[0]\n",
    "test_size = test_images.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(train_size, x, y, 1)\n",
    "test_images = test_images.reshape(test_size, x, y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential([\n",
    "    Conv2D(64, (3,3), activation='elu', input_shape=(x, y, 1)),\n",
    "    MaxPooling2D((2, 2)), \n",
    "    Conv2D(64, (3,3), activation='elu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(n_classes, activation='softplus')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(train_labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 106s 2ms/step - loss: 0.0398 - accuracy: 0.98770s - loss: 0.0398 - accuracy: 0.\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.0338 - accuracy: 0.9891\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.0269 - accuracy: 0.9915\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 110s 2ms/step - loss: 0.0250 - accuracy: 0.99140s - l\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 109s 2ms/step - loss: 0.0205 - accuracy: 0.9931\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 109s 2ms/step - loss: 0.0191 - accuracy: 0.9940\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 110s 2ms/step - loss: 0.0171 - accuracy: 0.9946\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 110s 2ms/step - loss: 0.0164 - accuracy: 0.99490s - loss: 0.0\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 111s 2ms/step - loss: 0.0144 - accuracy: 0.9954\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 112s 2ms/step - loss: 0.0151 - accuracy: 0.9956\n",
      "10000/10000 [==============================] - 7s 711us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06400028765004231, 0.988099992275238]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model.\n",
    "model2.compile(\n",
    "  optimizer='adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "model2.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=10,\n",
    "  batch_size=32\n",
    ")\n",
    "\n",
    "# Evaluate the model.\n",
    "model2.evaluate(\n",
    "  test_images,\n",
    "  to_categorical(test_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9881"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = np.argmax(model2.predict(test_images), axis=1)\n",
    "accuracy_score(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
